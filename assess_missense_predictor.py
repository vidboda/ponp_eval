import os
import sys
import re
import time
import yaml
import argparse
import tabix
import math

# script to assess missense prÃ©dictors
# based on curated datasets ("truth set") provided as text files
# format:
# hgvs_genomic\tvariant_class
# one file for pathogenic
# another for neutrals
# scores retrieved via a tabix query



def log(level, text):
    localtime = time.asctime( time.localtime(time.time()) )
    if level == 'ERROR':
        sys.exit('[{0}]: {1} - {2}'.format(level, localtime, text))
    print('[{0}]: {1} - {2}'.format(level, localtime, text))


def main():
    parser = argparse.ArgumentParser(
        description='Assess a missense variant predictor',
        usage='python assess_missense_predictor.py -c yaml_conf_file -p predictor_name -t truth_set'
    )
    parser.add_argument('-p', '--predictor', default='', required=True,
                        help='Predictor name (see config file)')
    parser.add_argument('-t', '--truth-set', default='', required=True,
                        help='prefix of truth set files (see config file)')
    parser.add_argument('-c', '--config-file', default='', required=True,
                        help='Config file yaml format. See assess_missense.yaml for examples.')

    args = parser.parse_args()
    predictor = args.predictor
    truth_set_name = args.truth_set
    # log('DEBUG', os.path.splitext(args.config_file))
    if os.path.splitext(args.config_file)[1] == '.yaml':
        # get config file
        config = yaml.safe_load(open(args.config_file))

        predictors = config['predictors']
        truth_set = config['truth_set']
        # log('DEBUG', truth_set)

        if predictor in predictors and \
                truth_set_name in truth_set['pathogenic'] and \
                truth_set_name in truth_set['neutral']:
            log('INFO', 'Predictor: {0}'.format(predictor))
            log('INFO', 'Predictor file path: {0}'.format(predictors[predictor]['file_path']))
            log('INFO', 'Pathogenic truth file: {0}{1}'.format(truth_set['path'], truth_set['pathogenic'][truth_set_name]['full_name']))
            log('INFO', 'Neutral truth file: {0}{1}'.format(truth_set['path'], truth_set['neutral'][truth_set_name]['full_name']))

            total_set_pathogenic, total_set_pathogenic_analysed, tp, fp, res_dict_pathogenic = assess_variant('pathogenic', predictor, truth_set_name, predictors, truth_set)
            # log('DEBUG', 'TP: {0} - FP: {1}'.format(tp, fp))
            total_set_neutral, total_set_neutral_analysed, tn, fn, res_dict_neutral = assess_variant('neutral', predictor, truth_set_name, predictors, truth_set)
            # log('DEBUG', 'TN: {0} - FN: {1}'.format(tn, fn))

            statistics = compute_statistics(tp, fp, tn, fn)
            # log('DEBUG', res_dict_pathogenic)
            perc_pathogenic_analysed = "%.2f" % ((total_set_pathogenic_analysed / total_set_pathogenic) * 100)
            log('INFO', '% of analysed pathogenic variants: {0} % ({1}/{2})'.format(
                perc_pathogenic_analysed,
                total_set_pathogenic_analysed,
                total_set_pathogenic
            ))
            # log('INFO', 'Total number of pathogenic variants tested: {}'.format(total_set_pathogenic))
            log('INFO', f'Number of pathogenic variants correctly predicted (TP): {tp}')
            log('INFO', f'Number of pathogenic variants uncorrectly predicted (FP): {fp}')

            perc_neutral_analysed = "%.2f" % ((total_set_neutral_analysed / total_set_neutral) * 100)
            log('INFO', '% of analysed neutral variants: {0} % ({1}/{2})'.format(
                perc_neutral_analysed,
                total_set_neutral_analysed,
                total_set_neutral
            ))
            #log('INFO', 'Total number of neutral variants tested: {}'.format(total_set_neutral))
            log('INFO', f'Number of neutral variants correctly predicted (TN): {tn}')
            log('INFO', f'Number of neutral variants uncorrectly predicted (FN): {fn}')

            for stat in statistics:
                log('INFO', '{0}: {1}'.format(stat, statistics[stat]))

            # for var in res_dict_neutral:
            #     log('DEBUG', '{0}: {1}_class: {2} - {3}_score: {4}'.format(
                    #     var,
                    #     truth_set_name,
                    #     res_dict_neutral[var]['{}_class'.format(truth_set_name)],
                    #     predictor,
                    #     res_dict_neutral[var]['{}_score'.format(predictor)])
                    # )

                            
def assess_variant(dataset, predictor, truth_set_name, predictors, truth_set):    
    total_set = 0
    total_set_analysed = 0
    true = 0
    false = 0
    res_dict = {}
    with open('{0}{1}'.format(truth_set['path'], truth_set[dataset][truth_set_name]['full_name'])) as variant_set: 
        for variant in variant_set:
            if re.search(r'^[^#]', variant):
                total_set += 1
                if match_obj := re.search(
                    r'^chr([\dXY]{1,2}):g\.(\d+)([ATGC])>([ATGC])\s+(.+)$',
                    variant,
                ):
                    chrom = match_obj[1]
                    pos = match_obj[2]
                    ref = match_obj[3]
                    alt = match_obj[4]
                    classif = match_obj[5]
                    # tabix query
                    tb = tabix.open(predictors[predictor]['file_path'])
                    query = "{0}:{1}-{2}".format(chrom, pos, pos)
                    try:
                        records = tb.querys(query)
                    except Exception as e:
                        # log('DEBUG', 'Tabix failed:{0} for variant {1}'.format(e.args, 'chr{0}:g.{1}{2}>{3}'.format(chrom, pos, ref, alt)))
                        continue
                    if records := tb.querys(query):
                        for record in records:
                            # log('DEBUG', 'tabix: {0} - variant: chr{1}:g.{2}{3}>{4}'.format(record, chrom, pos, ref, alt))
                            # log('DEBUG', 'File ref: {0} - ref: {1} - File alt:{2} - alt: {3}'.format(record[predictors[predictor]['ref_index']], ref, record[predictors[predictor]['alt_index']], alt))
                            if record[predictors[predictor]['ref_index']] == ref and \
                                    record[predictors[predictor]['alt_index']] == alt:
                                # treat multiple result as in dbNSFP
                                score = record[predictors[predictor]['score_index']]
                                # log('DEBUG', score)
                                if re.search(r';', score):
                                    # treat multiple result as in dbNSFP
                                    scores = re.split(';', score)
                                    score = max(scores)
                                    # log('DEBUG', '{0} - {1}'.format(scores, score))
                                if str(score) == '.':
                                    break
                                if dataset == 'neutral':
                                    if predictors[predictor]['score_sort'] == 'gt':
                                        if float(score) < float(predictors[predictor]['score_threshold']):
                                            # TP
                                            true += 1
                                        else:
                                            # FP
                                            false += 1
                                    elif float(score) > float(predictors[predictor]['score_threshold']):
                                        # TP
                                        true += 1
                                    else:
                                        # FP
                                        false += 1
                                elif dataset == 'pathogenic':
                                    if predictors[predictor]['score_sort'] == 'gt':
                                        if float(score) > float(predictors[predictor]['score_threshold']):
                                            # TP
                                            true += 1
                                        else:
                                            # FP
                                            false += 1
                                    elif float(score) < float(predictors[predictor]['score_threshold']):
                                        # TP
                                        true += 1
                                    else:
                                        # FP
                                        false += 1
                                res_dict[
                                    'chr{0}:g.{1}{2}>{3}'.format(
                                        chrom, pos, ref, alt
                                    )
                                ] = {
                                    f'{truth_set_name}_class': classif,
                                    f'{predictor}_score': record[
                                        predictors[predictor]['score_index']
                                    ],
                                }
                                total_set_analysed += 1
                                break
                else:
                    log('WARNING', f'Wrong format for variant line: {variant}')
            else:
                log('INFO', variant.rstrip())
    return total_set, total_set_analysed, true, false, res_dict

def compute_statistics(tp=0, fp=0, tn=0, fn=0):
    stats = {}
    if tp > 0 and fp > 0:
        stats['ppv'] =  "%.2f" % (tp / (tp + fp))
    if fn > 0 and tn > 0:
        stats['npv'] = "%.2f" % (tn / (fn + tn))
    if (tp + fn) > 0:
        stats['sensitivity'] = "%.2f" % (tp / (tp + fn))
    if (fp + tn) > 0:
        stats['specificity'] = "%.2f" % (tn / (fp + tn))
    if (tp + fp + fn + tn) > 0:
        stats['accuracy'] = "%.2f" % ((tp + tn) / (tp + fp + fn + tn))
    if ((tp + fn) * (tn + fp) * (tp + fp) * (tn + fn)) > 0:
        stats['mcc'] = "%.2f" % (((tp * tn) - (fp * fn)) / math.sqrt((tp + fn) * (tn + fp) * (tp + fp) * (tn + fn)))
    if 'ppv' in stats and 'sensitivity' in stats:
        stats['f-measure'] = "%.2f" % (2 * ((float(stats['ppv']) * float(stats['sensitivity']))) / ((float(stats['ppv']) + float(stats['sensitivity']))))
    return stats


if __name__ == '__main__':
    main()